{
  "name": "ollama-service",
  "version": "1.0.0",
  "description": "Ollama AI service for local LLM inference",
  "main": "index.ts",
  "scripts": {
    "dev": "bun --hot index.ts",
    "start": "bun index.ts"
  },
  "dependencies": {
    "hono": "^4.0.0",
    "@hono/node-server": "^1.8.0"
  },
  "devDependencies": {
    "@types/node": "^20.0.0",
    "bun-types": "^1.3.4"
  }
}